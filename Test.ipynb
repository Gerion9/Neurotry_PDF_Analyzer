{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PDF's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing Guide_Line_Dry Groseries.pdf...\n"
     ]
    },
    {
     "ename": "RateLimitError",
     "evalue": "Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01jf84n2z5fsq8x0s2q93chdqe` on : Limit 100000, Used 99209, Requested 1902. Please try again in 15m59.271s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': '', 'code': 'rate_limit_exceeded'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[33], line 143\u001b[0m\n\u001b[0;32m    141\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m pdf_file \u001b[38;5;129;01min\u001b[39;00m pdf_files:\n\u001b[0;32m    142\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mProcessing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpdf_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 143\u001b[0m     process_pdf(pdf_file)\n",
      "Cell \u001b[1;32mIn[33], line 92\u001b[0m, in \u001b[0;36mprocess_pdf\u001b[1;34m(pdf_path)\u001b[0m\n\u001b[0;32m     89\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(all_page_analyses), batch_size):\n\u001b[0;32m     90\u001b[0m         batch \u001b[38;5;241m=\u001b[39m all_page_analyses[i:i \u001b[38;5;241m+\u001b[39m batch_size]\n\u001b[1;32m---> 92\u001b[0m         summary_response \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39mchat\u001b[38;5;241m.\u001b[39mcompletions\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[0;32m     93\u001b[0m             messages\u001b[38;5;241m=\u001b[39m[\n\u001b[0;32m     94\u001b[0m                 {\n\u001b[0;32m     95\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[0;32m     96\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124mBased on these page analyses, extract and structure the guidelines into clear, specific items:\u001b[39m\n\u001b[0;32m     97\u001b[0m \n\u001b[0;32m     98\u001b[0m \u001b[38;5;132;01m{\u001b[39;00mjson\u001b[38;5;241m.\u001b[39mdumps(batch,\u001b[38;5;250m \u001b[39mindent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \n\u001b[0;32m    100\u001b[0m \u001b[38;5;124mCreate a detailed JSON structure that captures all unique guidelines mentioned. Group similar items and maintain specificity. Format as:\u001b[39m\n\u001b[0;32m    101\u001b[0m \u001b[38;5;130;01m{{\u001b[39;00m\n\u001b[0;32m    102\u001b[0m \u001b[38;5;124m  \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mguidelines\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m: [\u001b[39m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;124m    \u001b[39m\u001b[38;5;130;01m{{\u001b[39;00m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;124m      \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mproduct_type\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSpecific product or category this guideline applies to\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;124m      \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcategory\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTechnical specs|Requirements|Safety guidelines|Parameters\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\n\u001b[0;32m    106\u001b[0m \u001b[38;5;124m      \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdescription\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDetailed description of the guideline\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\n\u001b[0;32m    107\u001b[0m \u001b[38;5;124m      \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpriority\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhigh|medium|low\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    108\u001b[0m \u001b[38;5;124m    \u001b[39m\u001b[38;5;130;01m}}\u001b[39;00m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;124m  ]\u001b[39m\n\u001b[0;32m    110\u001b[0m \u001b[38;5;130;01m}}\u001b[39;00m\n\u001b[0;32m    111\u001b[0m \n\u001b[0;32m    112\u001b[0m \u001b[38;5;124mBe comprehensive but avoid duplicates. Preserve specific measurements, requirements, and technical details. Ensure each guideline clearly indicates which product type it applies to.\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m    113\u001b[0m                 }\n\u001b[0;32m    114\u001b[0m             ],\n\u001b[0;32m    115\u001b[0m             model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllama-3.3-70b-versatile\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    116\u001b[0m             response_format\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjson_object\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[0;32m    117\u001b[0m             temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m\n\u001b[0;32m    118\u001b[0m         )\n\u001b[0;32m    120\u001b[0m         batch_results \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(summary_response\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mcontent)\n\u001b[0;32m    121\u001b[0m         consolidated_guidelines\u001b[38;5;241m.\u001b[39mextend(batch_results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mguidelines\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\gairo\\Miniconda3\\Lib\\site-packages\\groq\\resources\\chat\\completions.py:298\u001b[0m, in \u001b[0;36mCompletions.create\u001b[1;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, logprobs, max_tokens, n, parallel_tool_calls, presence_penalty, response_format, seed, stop, stream, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[0;32m    157\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[0;32m    158\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    159\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    186\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[0;32m    187\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[0;32m    188\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    189\u001b[0m \u001b[38;5;124;03m    Creates a model response for the given chat conversation.\u001b[39;00m\n\u001b[0;32m    190\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    296\u001b[0m \u001b[38;5;124;03m      timeout: Override the client-level default timeout for this request, in seconds\u001b[39;00m\n\u001b[0;32m    297\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 298\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_post(\n\u001b[0;32m    299\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/openai/v1/chat/completions\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    300\u001b[0m         body\u001b[38;5;241m=\u001b[39mmaybe_transform(\n\u001b[0;32m    301\u001b[0m             {\n\u001b[0;32m    302\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: messages,\n\u001b[0;32m    303\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: model,\n\u001b[0;32m    304\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrequency_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: frequency_penalty,\n\u001b[0;32m    305\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunction_call\u001b[39m\u001b[38;5;124m\"\u001b[39m: function_call,\n\u001b[0;32m    306\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunctions\u001b[39m\u001b[38;5;124m\"\u001b[39m: functions,\n\u001b[0;32m    307\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogit_bias\u001b[39m\u001b[38;5;124m\"\u001b[39m: logit_bias,\n\u001b[0;32m    308\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: logprobs,\n\u001b[0;32m    309\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_tokens,\n\u001b[0;32m    310\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m\"\u001b[39m: n,\n\u001b[0;32m    311\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparallel_tool_calls\u001b[39m\u001b[38;5;124m\"\u001b[39m: parallel_tool_calls,\n\u001b[0;32m    312\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpresence_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: presence_penalty,\n\u001b[0;32m    313\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_format\u001b[39m\u001b[38;5;124m\"\u001b[39m: response_format,\n\u001b[0;32m    314\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseed\u001b[39m\u001b[38;5;124m\"\u001b[39m: seed,\n\u001b[0;32m    315\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop\u001b[39m\u001b[38;5;124m\"\u001b[39m: stop,\n\u001b[0;32m    316\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream,\n\u001b[0;32m    317\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemperature\u001b[39m\u001b[38;5;124m\"\u001b[39m: temperature,\n\u001b[0;32m    318\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtool_choice\u001b[39m\u001b[38;5;124m\"\u001b[39m: tool_choice,\n\u001b[0;32m    319\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtools\u001b[39m\u001b[38;5;124m\"\u001b[39m: tools,\n\u001b[0;32m    320\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_logprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_logprobs,\n\u001b[0;32m    321\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_p\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_p,\n\u001b[0;32m    322\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m: user,\n\u001b[0;32m    323\u001b[0m             },\n\u001b[0;32m    324\u001b[0m             completion_create_params\u001b[38;5;241m.\u001b[39mCompletionCreateParams,\n\u001b[0;32m    325\u001b[0m         ),\n\u001b[0;32m    326\u001b[0m         options\u001b[38;5;241m=\u001b[39mmake_request_options(\n\u001b[0;32m    327\u001b[0m             extra_headers\u001b[38;5;241m=\u001b[39mextra_headers, extra_query\u001b[38;5;241m=\u001b[39mextra_query, extra_body\u001b[38;5;241m=\u001b[39mextra_body, timeout\u001b[38;5;241m=\u001b[39mtimeout\n\u001b[0;32m    328\u001b[0m         ),\n\u001b[0;32m    329\u001b[0m         cast_to\u001b[38;5;241m=\u001b[39mChatCompletion,\n\u001b[0;32m    330\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    331\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mStream[ChatCompletionChunk],\n\u001b[0;32m    332\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\gairo\\Miniconda3\\Lib\\site-packages\\groq\\_base_client.py:1263\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[1;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1249\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[0;32m   1250\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1251\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1258\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1259\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m   1260\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[0;32m   1261\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[0;32m   1262\u001b[0m     )\n\u001b[1;32m-> 1263\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest(cast_to, opts, stream\u001b[38;5;241m=\u001b[39mstream, stream_cls\u001b[38;5;241m=\u001b[39mstream_cls))\n",
      "File \u001b[1;32mc:\\Users\\gairo\\Miniconda3\\Lib\\site-packages\\groq\\_base_client.py:955\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m    952\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    953\u001b[0m     retries_taken \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m--> 955\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[0;32m    956\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[0;32m    957\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m    958\u001b[0m     stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[0;32m    959\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[0;32m    960\u001b[0m     retries_taken\u001b[38;5;241m=\u001b[39mretries_taken,\n\u001b[0;32m    961\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\gairo\\Miniconda3\\Lib\\site-packages\\groq\\_base_client.py:1058\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[1;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1055\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m   1057\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1058\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1060\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[0;32m   1061\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[0;32m   1062\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1066\u001b[0m     retries_taken\u001b[38;5;241m=\u001b[39mretries_taken,\n\u001b[0;32m   1067\u001b[0m )\n",
      "\u001b[1;31mRateLimitError\u001b[0m: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01jf84n2z5fsq8x0s2q93chdqe` on : Limit 100000, Used 99209, Requested 1902. Please try again in 15m59.271s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': '', 'code': 'rate_limit_exceeded'}}"
     ]
    }
   ],
   "source": [
    "import base64\n",
    "import os\n",
    "import json\n",
    "from groq import Groq\n",
    "from PIL import Image\n",
    "import fitz  # PyMuPDF\n",
    "from pdf2image import convert_from_path\n",
    "\n",
    "# Function to encode the image\n",
    "def encode_image(image_path):\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode('utf-8')\n",
    "\n",
    "# Function to process PDF and extract images\n",
    "def process_pdf(pdf_path):\n",
    "    # Convert PDF pages to images\n",
    "    images = convert_from_path(pdf_path)\n",
    "    \n",
    "    # Store all guidelines analysis from all pages\n",
    "    all_page_analyses = []\n",
    "    \n",
    "    # Initialize Groq client\n",
    "    client = Groq(\n",
    "        api_key=os.environ.get(\"GROQ_API_KEY\"),\n",
    "    )\n",
    "    \n",
    "    # Process each page, skipping the first page\n",
    "    for i, image in enumerate(images[1:], start=1):\n",
    "        # Save page image temporarily\n",
    "        temp_path = f\"temp_page_{i}.jpg\"\n",
    "        image.save(temp_path, \"JPEG\")\n",
    "        \n",
    "        # Get base64 string\n",
    "        base64_image = encode_image(temp_path)\n",
    "\n",
    "        # Get detailed analysis from vision model\n",
    "        vision_response = client.chat.completions.create(\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\", \n",
    "                    \"content\": [\n",
    "                        {\n",
    "                            \"type\": \"text\", \n",
    "                            \"text\": \"\"\"Analyze this image with special emphasis on identifying the exact product type and category. Then describe all guidelines, specifications, and requirements. Focus on:\n",
    "\n",
    "1. Product type/category identification (CRITICAL):\n",
    "   - Exact product name and model\n",
    "   - Product category/family\n",
    "   - Brand and sub-brand if present\n",
    "   - Any product identifiers (SKU, model number, etc.)\n",
    "\n",
    "2. Technical specifications and requirements specific to the identified product\n",
    "3. Safety guidelines or warnings for this product type\n",
    "4. Product parameters or limitations\n",
    "5. Visual patterns or layouts suggesting guidelines\n",
    "6. Instructional or requirement text\n",
    "7. Tables or structured information with guidelines\n",
    "8. Icons/symbols indicating requirements\n",
    "\n",
    "IMPORTANT: For each guideline found, explicitly state which product or product category it applies to. If multiple products appear, organize information by product type. Be extremely precise in product identification and categorization. If no guideline information is found, state that clearly.\"\"\"\n",
    "                        },\n",
    "                        {\n",
    "                            \"type\": \"image_url\",\n",
    "                            \"image_url\": {\n",
    "                                \"url\": f\"data:image/jpeg;base64,{base64_image}\",\n",
    "                            },\n",
    "                        },\n",
    "                    ],\n",
    "                }\n",
    "            ],\n",
    "            model=\"llama-3.2-11b-vision-preview\",\n",
    "            temperature=0.2,  # Reduced temperature for more focused responses\n",
    "            stream=False\n",
    "        )\n",
    "        \n",
    "        # Store analysis from this page\n",
    "        page_analysis = {\n",
    "            \"page_number\": i + 1,\n",
    "            \"analysis\": vision_response.choices[0].message.content\n",
    "        }\n",
    "        all_page_analyses.append(page_analysis)\n",
    "                \n",
    "        # Clean up temp file\n",
    "        os.remove(temp_path)\n",
    "    # Process analyses in smaller batches to avoid token limits\n",
    "    batch_size = 3\n",
    "    consolidated_guidelines = []\n",
    "    \n",
    "    for i in range(0, len(all_page_analyses), batch_size):\n",
    "        batch = all_page_analyses[i:i + batch_size]\n",
    "        \n",
    "        summary_response = client.chat.completions.create(\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\", \n",
    "                    \"content\": f\"\"\"Based on these page analyses, extract and structure the guidelines into clear, specific items:\n",
    "\n",
    "{json.dumps(batch, indent=2)}\n",
    "\n",
    "Create a detailed JSON structure that captures all unique guidelines mentioned. Group similar items and maintain specificity. Format as:\n",
    "{{\n",
    "  \"guidelines\": [\n",
    "    {{\n",
    "      \"product_type\": \"Specific product or category this guideline applies to\",\n",
    "      \"category\": \"Technical specs|Requirements|Safety guidelines|Parameters\",\n",
    "      \"description\": \"Detailed description of the guideline\",\n",
    "      \"priority\": \"high|medium|low\"\n",
    "    }}\n",
    "  ]\n",
    "}}\n",
    "\n",
    "Be comprehensive but avoid duplicates. Preserve specific measurements, requirements, and technical details. Ensure each guideline clearly indicates which product type it applies to.\"\"\"\n",
    "                }\n",
    "            ],\n",
    "            model=\"llama-3.3-70b-versatile\",\n",
    "            response_format={\"type\": \"json_object\"},\n",
    "            temperature=0.2\n",
    "        )\n",
    "        \n",
    "        batch_results = json.loads(summary_response.choices[0].message.content)\n",
    "        consolidated_guidelines.extend(batch_results[\"guidelines\"])\n",
    "    # Save results to JSON file in pdf_analyses folder\n",
    "    output_filename = os.path.join('pdf_analyses', f\"guidelines_{os.path.splitext(os.path.basename(pdf_path))[0]}.json\")\n",
    "    \n",
    "    results = {\n",
    "        \"raw_page_analyses\": all_page_analyses,\n",
    "        \"consolidated_guidelines\": consolidated_guidelines\n",
    "    }\n",
    "    \n",
    "    with open(output_filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump(results, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    print(f\"\\nResults saved to {output_filename}\")\n",
    "\n",
    "# Process each PDF\n",
    "pdf_files = [\n",
    "    \"Guide_Line_Dry Groseries.pdf\",\n",
    "    \"Guide_Line_Pantallas.v.2.0.2019.pdf\"\n",
    "]\n",
    "\n",
    "for pdf_file in pdf_files:\n",
    "    print(f\"\\nProcessing {pdf_file}...\")\n",
    "    process_pdf(pdf_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Excel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing IF2 abarrotes secos.xlsx...\n",
      "\n",
      "Analyzing IF2 abarrotes secos.xlsx...\n",
      "\n",
      "Reading sheet: Hoja1\n",
      "Columns found: ['Unnamed: 0', 'Unnamed: 1', 'Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4', 'Unnamed: 5', 'Facet', 'Unnamed: 7', 'Unnamed: 8', 'Unnamed: 9', 'Información de GDA´S', 'Unnamed: 11', 'Unnamed: 12', 'Unnamed: 13', 'Unnamed: 14', 'Unnamed: 15', 'Unnamed: 16', 'Unnamed: 17', 'Unnamed: 18', 'Unnamed: 19', 'Unnamed: 20', 'Unnamed: 21', 'Unnamed: 22', 'Unnamed: 23', 'DIMENSIONES DEL ARTÍCULO CON EMPAQUE', 'Unnamed: 25', 'Unnamed: 26', 'Unnamed: 27', 'Unnamed: 28', 'DIMENSIONES DEL ARTÍCULO SIN EMPAQUE', 'Unnamed: 30', 'Unnamed: 31', 'Unnamed: 32', 'Unnamed: 33', 'Unnamed: 34', 'Unnamed: 35', 'Unnamed: 36', 'Unnamed: 37', 'Unnamed: 38', 'Unnamed: 39', 'Unnamed: 40', 'Unnamed: 41', 'Unnamed: 42', 'Unnamed: 43', 'Unnamed: 44', 'Unnamed: 45', 'Unnamed: 46', 'Unnamed: 47', 'Unnamed: 48', 'Unnamed: 49', 'Unnamed: 50', 'Unnamed: 51', 'Unnamed: 52', 'Unnamed: 53', 'Unnamed: 54', 'Unnamed: 55', 'Unnamed: 56', 'Unnamed: 57', 'Unnamed: 58', 'Unnamed: 59', 'Unnamed: 60', 'Unnamed: 61', 'Unnamed: 62', 'Unnamed: 63', 'Unnamed: 64', 'Unnamed: 65', 'Unnamed: 66', 'Unnamed: 67', 'Unnamed: 68', 'Unnamed: 69', 'Unnamed: 70', 'Unnamed: 71', 'Unnamed: 72', 'Unnamed: 73', 'Unnamed: 74', 'Unnamed: 75', 'Unnamed: 76', 'Unnamed: 77', 'Unnamed: 78', 'Unnamed: 79', 'Unnamed: 80', 'Unnamed: 81', 'Unnamed: 82', 'Unnamed: 83', 'Unnamed: 84', 'Unnamed: 85', 'Unnamed: 86', 'Unnamed: 87', 'Unnamed: 88', 'Unnamed: 89', 'Unnamed: 90', 'Unnamed: 91', 'Unnamed: 92', 'Unnamed: 93', 'Unnamed: 94', 'Unnamed: 95', 'Unnamed: 96', 'Unnamed: 97', 'Unnamed: 98', 'Unnamed: 99', 'Unnamed: 100', 'Unnamed: 101', 'Unnamed: 102', 'Unnamed: 103', 'Unnamed: 104']\n",
      "Number of rows: 3\n",
      "\n",
      "Reading sheet: Hoja2\n",
      "Columns found: []\n",
      "Number of rows: 0\n",
      "\n",
      "Reading sheet: Hoja3\n",
      "Columns found: []\n",
      "Number of rows: 0\n",
      "Analysis saved to excel_analyses\\IF2 abarrotes secos_analysis.json\n",
      "Restructured analysis saved to restructured_analyses\\IF2 abarrotes secos_restructured.json\n",
      "\n",
      "Processing IF2_Pantallas_Audio.xlsx...\n",
      "\n",
      "Analyzing IF2_Pantallas_Audio.xlsx...\n",
      "\n",
      "Reading sheet: 05 Electronicos\n",
      "Columns found: ['Campos Obligatorios para Todos los Productos', 'Unnamed: 1', 'Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4', 'Unnamed: 5', 'Unnamed: 6', 'Unnamed: 7', 'Unnamed: 8', 'Facets', 'Unnamed: 10', 'Unnamed: 11', 'Unnamed: 12', 'Unnamed: 13', 'Unnamed: 14', 'CARACTERÍSTICAS', 'Unnamed: 16', 'Unnamed: 17', 'Unnamed: 18', 'Unnamed: 19', 'Unnamed: 20', 'Unnamed: 21', 'Unnamed: 22', 'Unnamed: 23', 'Unnamed: 24', 'Unnamed: 25', 'Unnamed: 26', 'Unnamed: 27', 'Unnamed: 28', 'Unnamed: 29', 'Unnamed: 30', 'Unnamed: 31', 'Unnamed: 32', 'Unnamed: 33', 'Unnamed: 34', 'Unnamed: 35', 'Unnamed: 36', 'Unnamed: 37', 'DIMENSIONES DEL EMPAQUE', 'Unnamed: 39', 'Unnamed: 40', 'Unnamed: 41', 'Unnamed: 42', 'DIMENSIONES FUERA DEL EMPAQUE', 'Unnamed: 44', 'Unnamed: 45', 'Unnamed: 46']\n",
      "Number of rows: 4\n",
      "Analysis saved to excel_analyses\\IF2_Pantallas_Audio_analysis.json\n",
      "Restructured analysis saved to restructured_analyses\\IF2_Pantallas_Audio_restructured.json\n",
      "\n",
      "All analyses completed and saved in separate directories\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "from groq import Groq\n",
    "from typing import Dict, List\n",
    "\n",
    "# Create output directories if they don't exist\n",
    "output_dirs = ['excel_analyses', 'pdf_analyses', 'restructured_analyses']\n",
    "for dir_name in output_dirs:\n",
    "    os.makedirs(dir_name, exist_ok=True)\n",
    "\n",
    "def analyze_excel_file(excel_path):\n",
    "    print(f\"\\nAnalyzing {os.path.basename(excel_path)}...\")\n",
    "    \n",
    "    # Read all sheets from the Excel file\n",
    "    try:\n",
    "        excel_file = pd.ExcelFile(excel_path)\n",
    "        sheets = {}\n",
    "        \n",
    "        for sheet_name in excel_file.sheet_names:\n",
    "            print(f\"\\nReading sheet: {sheet_name}\")\n",
    "            \n",
    "            try:\n",
    "                # Read each sheet into a DataFrame, handling multi-header columns\n",
    "                df = pd.read_excel(excel_file, sheet_name=sheet_name)\n",
    "                \n",
    "                # Convert multi-index columns to string format\n",
    "                if isinstance(df.columns, pd.MultiIndex):\n",
    "                    df.columns = [' - '.join(str(level) for level in col if pd.notna(level)) \n",
    "                                for col in df.columns.values]\n",
    "                \n",
    "                # Basic analysis of sheet structure\n",
    "                print(f\"Columns found: {list(df.columns)}\")\n",
    "                print(f\"Number of rows: {len(df)}\")\n",
    "                \n",
    "                # Store sheet data with serializable column names\n",
    "                sheets[sheet_name] = {\n",
    "                    \"columns\": list(map(str, df.columns)),\n",
    "                    \"row_count\": len(df),\n",
    "                    \"header_levels\": df.columns.nlevels if isinstance(df.columns, pd.MultiIndex) else 1,\n",
    "                    \"sample_data\": df.head().to_dict('records')\n",
    "                }\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error reading sheet {sheet_name}: {str(e)}\")\n",
    "                \n",
    "        analysis = {\n",
    "            \"filename\": os.path.basename(excel_path),\n",
    "            \"sheet_count\": len(excel_file.sheet_names),\n",
    "            \"sheets\": sheets\n",
    "        }\n",
    "        \n",
    "        # Save individual Excel analysis\n",
    "        output_filename = os.path.join('excel_analyses', f\"{os.path.splitext(os.path.basename(excel_path))[0]}_analysis.json\")\n",
    "        with open(output_filename, 'w', encoding='utf-8') as f:\n",
    "            json.dump(analysis, f, indent=2, ensure_ascii=False)\n",
    "        print(f\"Analysis saved to {output_filename}\")\n",
    "        \n",
    "        return analysis\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"Error analyzing Excel file: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Initialize Groq client\n",
    "client = Groq(api_key=os.environ.get(\"GROQ_API_KEY\"))\n",
    "\n",
    "def analyze_and_restructure_json(json_data: Dict, filename: str) -> Dict:\n",
    "    \"\"\"\n",
    "    Use Groq LLM to analyze and restructure the JSON data into a clearer format.\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    Analyze this JSON data and restructure it into a clearer, more organized format:\n",
    "    \n",
    "    {json_data}\n",
    "    \n",
    "    Please organize it with the following considerations:\n",
    "    1. Group guidelines by product category\n",
    "    2. Sort by priority within each category\n",
    "    3. Create clear hierarchical relationships\n",
    "    4. Standardize property names\n",
    "    5. Add metadata section\n",
    "    \n",
    "    Return only the restructured JSON without any explanation.\n",
    "    \"\"\"\n",
    "    \n",
    "    chat_completion = client.chat.completions.create(\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        model=\"llama-3.3-70b-versatile\",\n",
    "        response_format={\"type\": \"json_object\"},\n",
    "        temperature=0.2\n",
    "    )\n",
    "    \n",
    "    # Parse the JSON string into a Python dictionary\n",
    "    restructured_json = json.loads(chat_completion.choices[0].message.content)\n",
    "    \n",
    "    # Save individual restructured analysis with pretty formatting\n",
    "    output_filename = os.path.join('restructured_analyses', f\"{os.path.splitext(filename)[0]}_restructured.json\")\n",
    "    with open(output_filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump(restructured_json, f, \n",
    "                 indent=4,\n",
    "                 ensure_ascii=False,\n",
    "                 sort_keys=True,\n",
    "                 separators=(',', ': '))\n",
    "    print(f\"Restructured analysis saved to {output_filename}\")\n",
    "    \n",
    "    return restructured_json\n",
    "\n",
    "# Process Excel files\n",
    "excel_files = [\n",
    "    r\"C:\\Users\\gairo\\OneDrive\\Documents\\Neurotry\\PDF_Analyzer\\IF2 abarrotes secos.xlsx\",\n",
    "    r\"C:\\Users\\gairo\\OneDrive\\Documents\\Neurotry\\PDF_Analyzer\\IF2_Pantallas_Audio.xlsx\"\n",
    "]\n",
    "\n",
    "# Process each Excel file individually\n",
    "for excel_file in excel_files:\n",
    "    print(f\"\\nProcessing {os.path.basename(excel_file)}...\")\n",
    "    \n",
    "    # Analyze Excel file\n",
    "    analysis = analyze_excel_file(excel_file)\n",
    "    \n",
    "    if analysis:\n",
    "        # Restructure the analysis\n",
    "        restructured_data = analyze_and_restructure_json(\n",
    "            {\"excel_file_analysis\": analysis}, \n",
    "            os.path.basename(excel_file)\n",
    "        )\n",
    "\n",
    "print(\"\\nAll analyses completed and saved in separate directories\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
